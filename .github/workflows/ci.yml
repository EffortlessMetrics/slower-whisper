name: CI

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  workflow_dispatch:
    inputs:
      benchmark_gate:
        description: 'Run benchmarks in gate mode (fail on regression)'
        required: false
        default: 'false'
        type: boolean
      benchmark_track:
        description: 'Benchmark track to run (asr, diarization, streaming, semantic, emotion, all)'
        required: false
        default: 'asr'
        type: choice
        options:
          - asr
          - diarization
          - streaming
          - semantic
          - emotion
          - all
  schedule:
    # Run full benchmarks weekly on Sunday at 2am UTC
    - cron: '0 2 * * 0'

# Cancel in-progress runs when a new workflow with the same group name is triggered
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  lint:
    name: Lint (ruff check)
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v6

      - name: Install uv
        uses: astral-sh/setup-uv@v7
        with:
          enable-cache: true
          cache-dependency-glob: |
            pyproject.toml
            uv.lock

      - name: Set up Python
        run: uv python install 3.12

      - name: Install dependencies
        run: uv sync --no-dev

      - name: Run ruff check
        run: uvx ruff@0.14.9 check transcription/ tests/ examples/ benchmarks/

  format:
    name: Format check (ruff format)
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v6

      - name: Install uv
        uses: astral-sh/setup-uv@v7
        with:
          enable-cache: true
          cache-dependency-glob: |
            pyproject.toml
            uv.lock

      - name: Set up Python
        run: uv python install 3.12

      - name: Check formatting
        run: uvx ruff@0.14.9 format --check transcription/ tests/ examples/ benchmarks/

  type-check:
    name: Type check (mypy)
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v6

      - name: Install uv
        uses: astral-sh/setup-uv@v7
        with:
          enable-cache: true
          cache-dependency-glob: |
            pyproject.toml
            uv.lock

      - name: Set up Python
        run: uv python install 3.12

      - name: Install dependencies
        run: uv sync --extra api --extra dev

      - name: Run mypy
        # Check transcription/ (typed island) plus strategic test modules
        # that validate public API contracts. Other tests are intentionally
        # untyped due to pytest patterns mypy can't verify.
        run: |
          uv run mypy transcription/ \
            tests/test_llm_utils.py \
            tests/test_writers.py \
            tests/test_turn_helpers.py \
            tests/test_audio_state_schema.py

  test:
    name: Test (Python ${{ matrix.python-version }})
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        python-version: ["3.12", "3.13"]

    steps:
      - uses: actions/checkout@v6

      - name: Install uv
        uses: astral-sh/setup-uv@v7
        with:
          enable-cache: true
          cache-dependency-glob: |
            pyproject.toml
            uv.lock

      - name: Set up Python ${{ matrix.python-version }}
        run: uv python install ${{ matrix.python-version }}

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y ffmpeg libsndfile1

      - name: Install dependencies
        run: uv sync --extra dev

      - name: Run tests with coverage
        run: |
          uv run pytest \
            --cov=transcription \
            --cov-report=xml \
            --cov-report=term-missing \
            -v \
            -m "not slow and not heavy" \
            -n auto

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v5
        if: matrix.python-version == '3.12'
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-umbrella
          fail_ci_if_error: false
        env:
          CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}

  test-heavy:
    name: Heavy tests (emotion models)
    runs-on: ubuntu-latest
    # Only run on main branch or when explicitly triggered
    if: github.ref == 'refs/heads/main' || github.event_name == 'workflow_dispatch'

    steps:
      - uses: actions/checkout@v6

      - name: Install uv
        uses: astral-sh/setup-uv@v7
        with:
          enable-cache: true
          cache-dependency-glob: |
            pyproject.toml
            uv.lock

      - name: Set up Python
        run: uv python install 3.12

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y ffmpeg libsndfile1

      - name: Install dependencies with emotion extras
        run: uv sync --extra emotion

      - name: Install test dependencies
        run: uv pip install pytest pytest-mock

      - name: Restore HuggingFace cache
        uses: actions/cache/restore@v5
        with:
          path: ~/.cache/huggingface
          key: hf-${{ runner.os }}-${{ hashFiles('pyproject.toml') }}
          restore-keys: |
            hf-${{ runner.os }}-

      - name: Run heavy tests
        run: |
          uv run pytest \
            -v \
            -m "heavy" \
            --tb=short
        continue-on-error: true  # Don't fail CI if heavy tests fail

      - name: Save HuggingFace cache
        if: always()
        continue-on-error: true
        uses: actions/cache/save@v5
        with:
          path: ~/.cache/huggingface
          key: hf-${{ runner.os }}-${{ hashFiles('pyproject.toml') }}

  test-integration:
    name: Integration tests
    runs-on: ubuntu-latest
    needs: [lint, format]  # Run after quick checks pass

    steps:
      - uses: actions/checkout@v6

      - name: Install uv
        uses: astral-sh/setup-uv@v7
        with:
          enable-cache: true
          cache-dependency-glob: |
            pyproject.toml
            uv.lock

      - name: Set up Python
        run: uv python install 3.12

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y ffmpeg libsndfile1

      - name: Install dependencies
        run: uv sync --extra api --extra dev

      - name: Run integration tests
        run: |
          uv run pytest tests/test_*integration*.py -v --tb=short

  bdd-library:
    name: BDD Library Contract (behavioral)
    runs-on: ubuntu-latest
    needs: [lint, format]  # Run after quick checks pass

    steps:
      - uses: actions/checkout@v6

      - name: Install uv
        uses: astral-sh/setup-uv@v7
        with:
          enable-cache: true
          cache-dependency-glob: |
            pyproject.toml
            uv.lock

      - name: Set up Python
        run: uv python install 3.12

      - name: Install system dependencies (ffmpeg required for BDD)
        run: |
          sudo apt-get update
          sudo apt-get install -y ffmpeg libsndfile1

      - name: Install dependencies
        run: uv sync --extra dev

      - name: Run library BDD scenarios
        run: |
          uv run pytest tests/steps/ -v -m "not slow and not requires_gpu" --tb=short

      - name: Upload BDD results
        if: always()
        uses: actions/upload-artifact@v6
        with:
          name: bdd-library-results
          path: .pytest_cache/

  bdd-api:
    name: BDD API Contract (REST service)
    runs-on: ubuntu-latest
    needs: [lint, format]  # Run after quick checks pass

    steps:
      - uses: actions/checkout@v6

      - name: Install uv
        uses: astral-sh/setup-uv@v7
        with:
          enable-cache: true
          cache-dependency-glob: |
            pyproject.toml
            uv.lock

      - name: Set up Python
        run: uv python install 3.12

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y ffmpeg libsndfile1

      - name: Install dependencies (with API deps)
        run: uv sync --extra api --extra dev

      - name: Run API BDD scenarios (smoke tests - hard gate)
        run: |
          uv run pytest features/ -v -m "api and smoke" --tb=short

      - name: Run API BDD scenarios (functional tests)
        run: |
          uv run pytest features/ -v -m "api and functional" --tb=short
        continue-on-error: true  # Functional tests recommended but not required

      - name: Upload API BDD results
        if: always()
        uses: actions/upload-artifact@v6
        with:
          name: bdd-api-results
          path: .pytest_cache/

  # Benchmark regression testing (report-only mode)
  benchmark:
    name: Benchmark (smoke test)
    runs-on: ubuntu-latest
    needs: [lint, format]  # Run after quick checks pass
    # Report-only mode: never fail CI, just report results
    continue-on-error: true

    steps:
      - uses: actions/checkout@v6

      - name: Install uv
        uses: astral-sh/setup-uv@v7
        with:
          enable-cache: true
          cache-dependency-glob: |
            pyproject.toml
            uv.lock

      - name: Set up Python
        run: uv python install 3.12

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y ffmpeg libsndfile1

      - name: Install dependencies
        run: uv sync --extra dev

      - name: Create benchmark results directory
        run: mkdir -p benchmark-results

      - name: Show benchmark infrastructure status
        run: |
          uv run slower-whisper benchmark status

      - name: List available benchmarks and baselines
        run: |
          uv run slower-whisper benchmark list
          echo ""
          uv run slower-whisper benchmark baselines

      - name: Run ASR benchmark smoke test (synthetic)
        id: benchmark-asr
        continue-on-error: true
        run: |
          # Generate a small synthetic test file for smoke testing
          # This tests the benchmark infrastructure without requiring large datasets
          python -c "
          import numpy as np
          import wave
          import os

          # Create a 2-second silent WAV file for infrastructure smoke test
          sample_rate = 16000
          duration = 2.0
          samples = int(sample_rate * duration)
          audio_data = np.zeros(samples, dtype=np.int16)

          os.makedirs('benchmark-results/smoke-data', exist_ok=True)
          with wave.open('benchmark-results/smoke-data/test.wav', 'w') as wav:
              wav.setnchannels(1)
              wav.setsampwidth(2)
              wav.setframerate(sample_rate)
              wav.writeframes(audio_data.tobytes())
          print('Created synthetic test audio for smoke test')
          "

          # Test that benchmark CLI is functional
          echo "Testing benchmark CLI functionality..."
          uv run slower-whisper benchmark --help > benchmark-results/benchmark-help.txt
          uv run slower-whisper benchmark run --help >> benchmark-results/benchmark-help.txt

          # Record benchmark infrastructure test results
          echo '{"test": "benchmark_cli_smoke", "status": "passed", "timestamp": "'$(date -Iseconds)'"}' > benchmark-results/smoke-test.json
          echo "Benchmark infrastructure smoke test completed"

      - name: Check baseline files exist
        run: |
          echo "Checking baseline files in benchmarks/baselines/..."
          ls -la benchmarks/baselines/ || echo "No baselines directory"
          find benchmarks/baselines -name "*.json" -type f 2>/dev/null | head -20 || echo "No baseline files found"

          # Validate baseline JSON format
          echo ""
          echo "Validating baseline file formats..."
          for f in benchmarks/baselines/*/*.json; do
            if [ -f "$f" ]; then
              echo "Checking $f..."
              python -c "import json; json.load(open('$f'))" && echo "  Valid JSON" || echo "  INVALID JSON"
            fi
          done

      - name: Generate benchmark summary
        if: always()
        run: |
          echo "## Benchmark Summary" > benchmark-results/SUMMARY.md
          echo "" >> benchmark-results/SUMMARY.md
          echo "**Run Date:** $(date -Iseconds)" >> benchmark-results/SUMMARY.md
          echo "**Commit:** ${{ github.sha }}" >> benchmark-results/SUMMARY.md
          echo "**Branch:** ${{ github.ref_name }}" >> benchmark-results/SUMMARY.md
          echo "" >> benchmark-results/SUMMARY.md

          echo "### Infrastructure Status" >> benchmark-results/SUMMARY.md
          uv run slower-whisper benchmark status >> benchmark-results/SUMMARY.md 2>&1 || true
          echo "" >> benchmark-results/SUMMARY.md

          echo "### Baseline Files" >> benchmark-results/SUMMARY.md
          echo '```' >> benchmark-results/SUMMARY.md
          find benchmarks/baselines -name "*.json" -type f 2>/dev/null || echo "No baselines found"
          echo '```' >> benchmark-results/SUMMARY.md

          echo "" >> benchmark-results/SUMMARY.md
          echo "### Smoke Test Result" >> benchmark-results/SUMMARY.md
          if [ -f "benchmark-results/smoke-test.json" ]; then
            echo '```json' >> benchmark-results/SUMMARY.md
            cat benchmark-results/smoke-test.json >> benchmark-results/SUMMARY.md
            echo '```' >> benchmark-results/SUMMARY.md
          else
            echo "Smoke test did not complete" >> benchmark-results/SUMMARY.md
          fi

          cat benchmark-results/SUMMARY.md

      - name: Upload benchmark results
        if: always()
        uses: actions/upload-artifact@v6
        with:
          name: benchmark-results
          path: benchmark-results/
          retention-days: 30

  # Full benchmark with gate mode (scheduled or manual trigger)
  benchmark-gate:
    name: Benchmark (gate mode)
    runs-on: ubuntu-latest
    # Only run on schedule, main branch push, or manual trigger with gate enabled
    if: |
      github.event_name == 'schedule' ||
      (github.event_name == 'workflow_dispatch' && inputs.benchmark_gate == 'true') ||
      (github.event_name == 'push' && github.ref == 'refs/heads/main')
    needs: [lint, format, test]

    steps:
      - uses: actions/checkout@v6

      - name: Install uv
        uses: astral-sh/setup-uv@v7
        with:
          enable-cache: true
          cache-dependency-glob: |
            pyproject.toml
            uv.lock

      - name: Set up Python
        run: uv python install 3.12

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y ffmpeg libsndfile1

      - name: Install dependencies
        run: uv sync --extra dev

      - name: Restore benchmark cache
        uses: actions/cache/restore@v5
        with:
          path: |
            ~/.cache/huggingface
            benchmarks/.cache
          key: benchmark-${{ runner.os }}-${{ hashFiles('pyproject.toml') }}
          restore-keys: |
            benchmark-${{ runner.os }}-

      - name: Create benchmark results directory
        run: mkdir -p benchmark-results

      - name: Determine benchmark track
        id: track
        run: |
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            TRACK="${{ inputs.benchmark_track || 'asr' }}"
          else
            TRACK="asr"
          fi
          echo "track=$TRACK" >> $GITHUB_OUTPUT
          echo "Running benchmark track: $TRACK"

      - name: Run benchmark with gate mode (ASR smoke)
        id: benchmark-gate-run
        run: |
          echo "Running ASR benchmark in gate mode..."
          uv run slower-whisper benchmark run \
            --track asr \
            --dataset smoke \
            --gate \
            --threshold wer=0.10 \
            --threshold cer=0.05 \
            --output benchmark-results/asr-gate-results.json \
            2>&1 | tee benchmark-results/gate-output.log
        continue-on-error: ${{ github.event_name != 'schedule' }}

      - name: Compare with baselines
        id: baseline-compare
        run: |
          echo "Comparing with stored baselines..."
          uv run slower-whisper benchmark compare \
            --track asr \
            --dataset smoke \
            --output benchmark-results/comparison.json \
            2>&1 | tee -a benchmark-results/gate-output.log
        continue-on-error: true

      - name: Generate gate summary
        if: always()
        run: |
          echo "## Benchmark Gate Summary" > benchmark-results/GATE_SUMMARY.md
          echo "" >> benchmark-results/GATE_SUMMARY.md
          echo "**Run Date:** $(date -Iseconds)" >> benchmark-results/GATE_SUMMARY.md
          echo "**Commit:** ${{ github.sha }}" >> benchmark-results/GATE_SUMMARY.md
          echo "**Trigger:** ${{ github.event_name }}" >> benchmark-results/GATE_SUMMARY.md
          echo "**Track:** ${{ steps.track.outputs.track }}" >> benchmark-results/GATE_SUMMARY.md
          echo "" >> benchmark-results/GATE_SUMMARY.md

          if [ "${{ steps.benchmark-gate-run.outcome }}" == "success" ]; then
            echo "### Result: ✅ PASSED" >> benchmark-results/GATE_SUMMARY.md
            echo "All benchmarks within regression thresholds." >> benchmark-results/GATE_SUMMARY.md
          else
            echo "### Result: ⚠️ ATTENTION NEEDED" >> benchmark-results/GATE_SUMMARY.md
            echo "Benchmark may have regressed or had issues. Review output below." >> benchmark-results/GATE_SUMMARY.md
          fi

          echo "" >> benchmark-results/GATE_SUMMARY.md
          echo "### Output Log" >> benchmark-results/GATE_SUMMARY.md
          echo '```' >> benchmark-results/GATE_SUMMARY.md
          cat benchmark-results/gate-output.log 2>/dev/null || echo "No output log available"
          echo '```' >> benchmark-results/GATE_SUMMARY.md

          cat benchmark-results/GATE_SUMMARY.md

      - name: Save benchmark cache
        if: always()
        continue-on-error: true
        uses: actions/cache/save@v5
        with:
          path: |
            ~/.cache/huggingface
            benchmarks/.cache
          key: benchmark-${{ runner.os }}-${{ hashFiles('pyproject.toml') }}

      - name: Upload gate results
        if: always()
        uses: actions/upload-artifact@v6
        with:
          name: benchmark-gate-results
          path: benchmark-results/
          retention-days: 90

      - name: Fail on regression (scheduled runs only)
        if: github.event_name == 'schedule' && steps.benchmark-gate-run.outcome == 'failure'
        run: |
          echo "::error::Benchmark regression detected in scheduled run!"
          exit 1

  # Documentation build
  docs:
    name: Build docs
    runs-on: ubuntu-latest
    needs: [lint, format]  # Run after quick checks pass

    steps:
      - uses: actions/checkout@v6

      - name: Install uv
        uses: astral-sh/setup-uv@v7
        with:
          enable-cache: true
          cache-dependency-glob: |
            pyproject.toml
            uv.lock

      - name: Set up Python
        run: uv python install 3.12

      - name: Install docs dependencies
        run: uv sync --extra docs --no-dev

      - name: Build documentation
        run: ./scripts/docs-build.sh --strict

      - name: Upload documentation
        uses: actions/upload-artifact@v6
        with:
          name: docs-html
          path: docs/_build/html/
          retention-days: 14

  # Summary job that all required checks depend on
  ci-success:
    name: CI Success
    if: always()
    needs:
      - lint
      - format
      - type-check
      - test
      - test-integration
      - bdd-library
      - bdd-api
      - benchmark
      - docs
    runs-on: ubuntu-latest
    steps:
      - name: Check all jobs
        run: |
          if [[ "${{ needs.lint.result }}" == "failure" ]] || \
             [[ "${{ needs.format.result }}" == "failure" ]] || \
             [[ "${{ needs.type-check.result }}" == "failure" ]] || \
             [[ "${{ needs.test.result }}" == "failure" ]] || \
             [[ "${{ needs.test-integration.result }}" == "failure" ]] || \
             [[ "${{ needs.bdd-library.result }}" == "failure" ]] || \
             [[ "${{ needs.bdd-api.result }}" == "failure" ]] || \
             [[ "${{ needs.docs.result }}" == "failure" ]]; then
            echo "One or more required jobs failed"
            exit 1
          fi

          # Report benchmark status (informational only for PR, gated for schedule)
          echo ""
          if [[ "${{ needs.benchmark.result }}" == "failure" ]]; then
            echo "⚠️ Benchmark smoke test had issues (informational only)"
          elif [[ "${{ needs.benchmark.result }}" == "success" ]]; then
            echo "✅ Benchmark smoke test passed"
          else
            echo "ℹ️ Benchmark smoke test: ${{ needs.benchmark.result }}"
          fi

          echo ""
          echo "All required jobs passed!"
          echo ""
          echo "✅ Lint passed"
          echo "✅ Format check passed"
          echo "✅ Type check passed"
          echo "✅ Unit tests passed"
          echo "✅ Integration tests passed"
          echo "✅ Library BDD contract enforced"
          echo "✅ API BDD contract enforced"
          echo "✅ Documentation builds cleanly"
          echo ""
          echo "Note: Benchmark gate mode runs on schedule (weekly) and manual trigger."
          echo "      Use workflow_dispatch with benchmark_gate=true to test gate mode."
