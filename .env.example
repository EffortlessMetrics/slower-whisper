# =============================================================================
# Environment Variables for Docker Compose
# =============================================================================
# Copy this file to .env and customize for your environment
# Values here are used by docker-compose.yml
#
# Usage:
#   cp .env.example .env
#   # Edit .env with your settings
#   docker compose up transcribe-gpu
#
# =============================================================================

# -----------------------------------------------------------------------------
# Transcription Configuration
# -----------------------------------------------------------------------------

# Whisper model to use
# Options: tiny, base, small, medium, large-v2, large-v3
# Larger models are more accurate but slower and require more VRAM
SLOWER_WHISPER_MODEL=large-v3

# Device to use for transcription
# Options: cuda, cpu
# GPU (cuda) is 10-50x faster but requires NVIDIA GPU
SLOWER_WHISPER_DEVICE=cuda

# Language for transcription
# Options: auto (auto-detect), en, es, fr, de, ja, zh, etc.
# See https://github.com/openai/whisper for full list
SLOWER_WHISPER_LANGUAGE=auto

# Compute type for inference
# GPU options: float16, int8_float16, int8
# CPU options: int8, int8_float32
# Lower precision is faster but slightly less accurate
SLOWER_WHISPER_COMPUTE_TYPE=float16

# -----------------------------------------------------------------------------
# GPU Configuration
# -----------------------------------------------------------------------------

# Which GPU to use (if you have multiple GPUs)
# Options: 0, 1, 2, etc., or "all"
CUDA_VISIBLE_DEVICES=0

# Number of OpenMP threads (parallel CPU processing)
# Adjust based on your CPU cores (usually 4-8 is optimal)
OMP_NUM_THREADS=4

# MKL threads for CPU-based inference
# Only relevant when SLOWER_WHISPER_DEVICE=cpu
MKL_NUM_THREADS=8

# -----------------------------------------------------------------------------
# Audio Enrichment Configuration
# -----------------------------------------------------------------------------

# Enable prosody extraction (pitch, energy, rate)
ENRICH_PROSODY=true

# Enable emotion recognition
ENRICH_EMOTION=true

# -----------------------------------------------------------------------------
# Paths Configuration
# -----------------------------------------------------------------------------

# Override default cache directory for HuggingFace models
# Uncomment to use custom location
# HF_HOME=/custom/path/to/huggingface

# Override default cache directory for Whisper models
# Uncomment to use custom location
# WHISPER_CACHE=/custom/path/to/whisper

# User home directory (for volume mounts)
# Usually detected automatically
# HOME=/home/username

# -----------------------------------------------------------------------------
# Development Configuration
# -----------------------------------------------------------------------------

# Enable development mode (verbose logging, debug features)
SLOWER_WHISPER_DEV_MODE=0

# Log level for development
# Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Enable debug mode for slower-whisper
SLOWER_WHISPER_DEBUG=0

# -----------------------------------------------------------------------------
# Resource Limits (optional)
# -----------------------------------------------------------------------------

# Memory limit for containers (e.g., 8G, 16G)
# Uncomment to override docker-compose.yml defaults
# MEMORY_LIMIT=8G

# CPU limit for containers (e.g., 4, 8, 16)
# MEMORY_LIMIT=8G

# CPU limit for containers (e.g., 4, 8, 16)
# Uncomment to override docker-compose.yml defaults
# CPU_LIMIT=8

# -----------------------------------------------------------------------------
# Example Configurations
# -----------------------------------------------------------------------------

# Fast transcription (lower quality, faster):
#   SLOWER_WHISPER_MODEL=base
#   SLOWER_WHISPER_COMPUTE_TYPE=int8
#   SLOWER_WHISPER_DEVICE=cuda

# High quality transcription (slower):
#   SLOWER_WHISPER_MODEL=large-v3
#   SLOWER_WHISPER_COMPUTE_TYPE=float16
#   SLOWER_WHISPER_DEVICE=cuda

# CPU-only transcription (no GPU):
#   SLOWER_WHISPER_MODEL=medium
#   SLOWER_WHISPER_DEVICE=cpu
#   SLOWER_WHISPER_COMPUTE_TYPE=int8

# Japanese transcription:
#   SLOWER_WHISPER_LANGUAGE=ja
#   SLOWER_WHISPER_MODEL=large-v3

# Multi-GPU setup (use second GPU):
#   CUDA_VISIBLE_DEVICES=1
