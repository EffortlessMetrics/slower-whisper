# Docker Compose configuration for slower-whisper FastAPI service
# This provides a production-ready deployment with health checks and persistent model caching

version: '3.8'

services:
  # =============================================================================
  # FastAPI Service (CPU-based)
  # =============================================================================
  api:
    image: slower-whisper:api
    build:
      context: .
      dockerfile: Dockerfile.api
      args:
        # Use api-full for complete features (recommended)
        # Use api-min for transcription-only (smaller image)
        INSTALL_MODE: api-full
    container_name: slower-whisper-api
    ports:
      - "8000:8000"
    environment:
      # OpenMP thread pool size (adjust based on CPU cores)
      - OMP_NUM_THREADS=4
      # Optional: Override default uvicorn workers
      # - UVICORN_WORKERS=4
    volumes:
      # Persistent model cache to avoid re-downloading Whisper and emotion models
      - model-cache:/home/appuser/.cache/huggingface
      # Optional: Mount for temporary file processing
      - /tmp/slower-whisper-uploads:/tmp/uploads
    healthcheck:
      test: ["CMD-SHELL", "python -c \"import requests; requests.get('http://localhost:8000/health', timeout=5)\" || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    restart: unless-stopped
    # Resource limits (adjust based on your needs)
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 8G
        reservations:
          cpus: '2.0'
          memory: 4G

  # =============================================================================
  # Optional: Nginx reverse proxy for production
  # =============================================================================
  # Uncomment this section to add nginx for SSL termination, rate limiting, etc.
  #
  # nginx:
  #   image: nginx:alpine
  #   container_name: slower-whisper-nginx
  #   ports:
  #     - "80:80"
  #     - "443:443"
  #   volumes:
  #     - ./nginx.conf:/etc/nginx/nginx.conf:ro
  #     - ./ssl:/etc/nginx/ssl:ro
  #   depends_on:
  #     - api
  #   restart: unless-stopped

volumes:
  # Persistent volume for HuggingFace model cache
  # This prevents re-downloading ~6GB of models on container restart
  model-cache:
    driver: local

# =============================================================================
# Usage Instructions:
# =============================================================================
#
# Build and start the service:
#   docker-compose -f docker-compose.api.yml up --build -d
#
# View logs:
#   docker-compose -f docker-compose.api.yml logs -f api
#
# Stop the service:
#   docker-compose -f docker-compose.api.yml down
#
# Stop and remove volumes (clears model cache):
#   docker-compose -f docker-compose.api.yml down -v
#
# Rebuild the service:
#   docker-compose -f docker-compose.api.yml build --no-cache api
#
# Scale workers (for load balancing):
#   docker-compose -f docker-compose.api.yml up --scale api=3 -d
#
# =============================================================================
# API Endpoints:
# =============================================================================
#
# Health check:
#   curl http://localhost:8000/health
#
# OpenAPI documentation (Swagger UI):
#   http://localhost:8000/docs
#
# ReDoc documentation:
#   http://localhost:8000/redoc
#
# Transcribe audio:
#   curl -X POST -F "audio=@interview.mp3" \
#     "http://localhost:8000/transcribe?model=large-v3&language=en&device=cpu"
#
# Enrich transcript:
#   curl -X POST \
#     -F "transcript=@transcript.json" \
#     -F "audio=@audio.wav" \
#     "http://localhost:8000/enrich?enable_prosody=true&enable_emotion=true"
#
